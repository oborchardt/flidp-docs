\chapter{Related work}

\textcite{mcmahan:2018} entwickeln eine Version des FedAvg-Algorithmus, die Differential Privacy Garantien erfüllt, um lokale Language Modelle zu trainieren. Darin begrenzen sie die $\ell_2$-Norm der Gradienten der Clients und können so die Sensitivität der Aggregation der Clients abschätzen. Einen ähnlichen Ansatz liefern \textcite{geyer:2017}, allerdings berechnen sie die Sensitivität der einzelnen Gradienten basierend auf der Median-Norm.

\textcite{boenisch:2023} untersuchen in ihrer Arbeit zwei Ansätze für das Training mit individuellen Privacy Budgets. Bei einem wird das Rauschen, das auf die Gradienten addiert wird, mit individuellen \textit{noise multipliers} an die individuellen Budgets angepasst. Bei dem anderen Ansatz wird mit individuellen \textit{sampling rates} gearbeitet. Beide Ansätze werden mit verschiedenen Budgets und verschiedenen Verteilungen der Budgets evaluiert. Dabei liefert der zweite Ansatz bessere Ergebnisse.

\textcite{aldaghri:2023} stellen in ihrer Arbeit einen Algorithmus zum Training mit individuellen Privacy Budgets im Federated Learning vor. Im ersten Schritt leiten sie einen Algorithmus für lineare Regressionen her, im zweiten einen darauf basierenden generellen Algorithmus. Ihr Algorithmus arbeitet mit individuellen \textit{noise multipliers} und einer uniformen \textit{sampling rate}. Anders als \citeauthor{boenisch:2023} evaluieren sie ihren Algorithmus nur mit zwei Abstufungen von Privacy Budgets, nämlich privaten und nicht privaten Clients.

\textcite{yang:2021} unterscheiden in ihrer Arbeit zwischen Algorithmen mit globaler und lokaler DP im Federated Learning. Letztere schützen nicht nur das entstehende globale Modell, sondern auch die Gradientenupdates der Clients gegenüber dem aggregierenden Server. Sie entwickeln einen Algorithmus, der lokale DP für individuelle Privacy Budgets erfüllt.

\textcite{liu:2021} merken in ihrer Arbeit an, dass während des Trainings das Modell einen Bias bezüglich der Daten von weniger privaten Clients entwickeln kann. In ihrem Ansatz versuchen sie daher anstelle verrauschter Updates die "richtigen" Informationen privater Clients zu extrahieren. Dazu extrahieren sie den wichtigsten Unterraum der Updates öffentlicher Clients und projizieren die Updates privater Clients auf diesen.

\textcite{shen:2023} entwickeln in ihrer Arbeit ebenfalls einen Algorithmus, der lokale DP bei individuellen Privacy Budgets erfüllt. Ihre Definition von Privacy ist aus \textcite{chen:2016}. Bei dieser wird ein Bereich $\tau$ eingeführt, für dessen Werte dann $\epsilon$-DP erfüllt wird. Sie evaluieren ihren Ansatz und vergleichen ihn mit FedAvg \parencite{mcmahan:2016} als oberer Schranke und PLU-FedOA aus \textcite{yang:2021}. Hier liegt die Performance ihres Algorithmus unter, aber nahe an FedAvg und über PLU-FedOA.
