\chapter{Related work}\label{chap:related-work}

\begin{itemize}
	%	\item Konvergenz des FL verbessern durch Algorithmen
	%	\begin{itemize}
		%		\item Varianz reduzieren (SCAFFOLD), Momente nutzen, ... (sehr viele Ansätze werden in \textcite[p.26ff]{kairouz:2021} erwähnt)
		%	\end{itemize}
	\item Ansätze für personalisierte Modelle (bei non-iid Datensätzen) \parencite[p.28ff]{kairouz:2021} bzw. auch die Idee Globale Modelle durch Kontext anzureichern um bessere Vorhersagen für einzelne Nutzer zu bekommen
	\item Ergebnisse von anderen Arbeiten auf den gleichen Datensätzen
	%	\item Übertragung von non-FL Verfahren auf FL und die Probleme die dabei entstehen (z.B. beim Hyperparametertuning) (p.30ff)
	%	\item Kompression / Effizienzsteigerung von FL (p.32ff.)
	%	\item sehr interessanter Absatz zu Kompatibilität von DP und Kompressionstechniken (p.33)
	%	\item DP in FL (p. 44ff), vor allem die Referenzen zu Hybrid DP tun das was ich machen will aber anders
	%	\item Kapitel 4.3 (p.48 ff) für mein Szenario mit einem vertrauenswürdigen Server
	%	\item p.50 hat vieles was bei mir in Future Work kann
	%	\item p.54 hat ganz klaren Vergleich zwischen Local und Central DP!
\end{itemize}

In diesem Kapitel beschreibe ich den Stand der Forschung in den für meine Arbeit relevanten Forschungsgebieten. Da sie mehrere Problemfelder umfasst, werde ich das Kapitel im folgenden in drei Teile unterteilen. In \autoref{sec:rw-idp-ml} gehe ich auf Arbeiten ein, die individualisierte Differential Privacy aus \autoref{fund-idp} auf Machine Learning Algorithmen übertragen. In \autoref{sec:rw-fl} beschreibe ich den Stand der Forschung in Bezug auf Fragen des Federated Learning, beispielsweise wie mit unterschiedlichen Verteilungen in den Trainingsdatensätzen umgegangen werden kann. In \autoref{sec:rw-fldp} gehe ich auf Arbeiten ein, welche die Notwendigkeit von Differential Privacy im Federated Learning belegen und auf Algorithmen, welche Differential Privacy mit einheitlichen Privacy-Budgets im Federated Learning umsetzen, um die Basis für Algorithmen mit individualisierten Budgets zu legen, die ich in \autoref{sec:rw-flidp} beschreibe.

\section{Individualized Differential Privacy in Machine Learning}\label{sec:rw-idp-ml}

Wie in \autoref{sec:fund-dp-in-ml} angedeutet, ist die Arbeit mit individualisierten Privacy Budgets eine Möglichkeit, die Nützlichkeit eines Machine Learning Modells unter Einhaltung der Privacy Budgets der Nutzer zu erhöhen.

\textcite{boenisch:2023} untersuchen in ihrer Arbeit das Training mit individuellen Privacy Budgets. Sie stellen zwei Verfahren vor: \textbf{SAMPLE} und \textbf{SCALE}. \textbf{SAMPLE} basiert auf \textcite{jorgensen:2015} und erreicht die individuellen Budgets indem Datenpunkte im Training mit unterschiedlichen Wahrscheinlichkeiten gezogen werden. Die Wahrscheinlichkeiten werden etwas anders berechnet als bei \textcite{jorgensen:2015}. \textbf{SCALE} ist inspiriert von \textcite{alaggan:2016}. Anders als bei deren \textit{Stretching Mechanism} werden allerdings nicht die Datenpunkte selbst, sondern das addierte Rauschen skaliert. 

Beide Ansätze werden mit verschiedenen Budgets und Verteilungen der Budgets evaluiert. Sie teilen die Trainingsdaten in drei Gruppen ein (niedriges, mittleres und hohe Privacy Budget). Darüber hinaus testen sie zwei Szenarien, bei denen sich die Größen der drei Gruppen unterscheiden. Die erste Verteilung lässt mehr Trainingspunkte mit hohem und mittleren Budget zu ($34\%$, $43\%$, $23\%$), die andere weniger ($54\%$, $37\%$, $9\%$). Wie bei \textcite{jorgensen:2015} sind die Ergebnisse im ersten Fall besser und werden im zweiten Fall schlechter. In der Evaluation schneidet \textbf{SAMPLE} in allen Experimenten bis auf einem etwas besser ab als \textbf{SCALE}.

Auch für PATE haben \textcite{boenisch:2023b} mit IPATE eine individualisierte Version entwickelt. Sie erforschen zwei unterschiedliche Ansätze: \textit{upsampling} und \textit{weighting}. Der erste dupliziert Datenpunkte in Abhängigkeit von ihrem Privacy-Budget. Damit ist die Disjunktheit der Daten der einzelnen \textit{Teacher Models} nicht mehr gegeben. Mit \textit{weighting} gewichten sie die Stimmen der unterschiedlichen \textit{Teacher Models} unterschiedlich. Sie analysieren die Privacy-Kosten von PATE und IPATE und können mit dem individualisierten Ansatz deutlich mehr Trainingsdaten für das \textit{Student Model} klassifizieren bis die Privacy-Budgets aufgebraucht sind. Außerdem zeigen sie, dass die zusätzlichen Trainingsdaten einen positiven Effekt auf die Genauigkeit des \textit{Student Models} hat.

Die Privacy-Budgets und die Einteilung der Daten in drei Gruppen \citeauthor{boenisch:2023} basieren auf zwei Studien aus dem Jahr 2005 \cite{jensen:2005, acquisti:2005}. Beide untersuchen die Privacy-Präferenzen im Internet. Auch in älteren Studien dazu wird die Einteilung in drei Gruppen vorgenommen, ist also eine gängige Einteilung \cite{westin:1998}.

Bei \textcite{jensen:2005} erfolgt die Einteilung von Befragten basierend auf fünf Fragen. Beantworten sie vier davon mit einem Fokus auf Privatheit, kommen sie in die strengste der drei Gruppen. Beantworten sie keine der Fragen dementsprechend werden sie der Gruppe mit dem größten Privacy Budget zugeordnet. Der Rest kommt in die mittlere Gruppe. So kommen sie auf die Verteilung von ($34\%$, $43\%$, $23\%$) (von strenger zu weniger streng). 

Sie merken an, dass Befragte bei ihren Antworten ihre Sorge um die Privatheit und ihren Willen zu handeln stärker zum Ausdruck bringen, als sie tatsächlich sind. Diese These untermauern sie, indem sie Fragen ob eine Person sich mit Technologien für das Internet-Tracking (Cookies, Web-Bugs, ...) auskennt. Zur Überprüfung haben sie für die Themen Wissensfragen vorbereitet. Nur ein kleiner Teil von den Personen, die angegeben haben sich mit dem Thema auszukennen konnten die Wissensfragen richtig beantworten.

\textcite{acquisti:2005} haben ebenfalls eine Umfrage dazu durchgeführt und bemerken ebenfalls eine Diskrepanz zwischen der Angegebenen Sorge um Privatheit und bisher unternommenen Schritten um diese zu verbessern. Diese Unterschiede in beiden Studien legen Nahe, dass es wichtig ist, den Schutz der Privatheit einfach zu ermöglichen und so zugänglich zu halten, dass die Endnutzer sich nicht weiter mit den Details auskennen müssen. \textcite{jorgensen:2015} plädieren ebenfalls dafür, merken aber an, dass die genaue Wahl eines Privacy-Budgets für ein gegebenes System ein offenes Problem ist.

% noble:2023 Seite 2 für mehr related work!
\section{Federated Learning}\label{sec:rw-fl}

Wie in \autoref{fund-fl} angedeutet, sind \texttt{FedSGD} und vor allem \texttt{FedAvg} die Standardalgorithmen im Federated Learning. Beide sind relativ simpel und in Federated Learning Frameworks wie \textit{tensorflow federated} und \textit{flower} enthalten. In \autoref{alg:fedavg} ist der Pseudocode für \texttt{FedAvg} aus \cite{mcmahan:2016} dargestellt. 


\begin{algorithm}[tb]
	\caption{FederatedAveraging (\texttt{FedAvg})}
	\label{alg:fedavg}
	\SetKwFunction{ClientUpdate}{ClientUpdate}
	\SetKwFunction{Server}{Server}
	\SetKwProg{Fn}{Function}{:}{}
	\Fn{\Server{}}{
		initialize $w_0$; \\
		\For{each round $t = 1, 2, \dots$}{
			$S_t \leftarrow$ random set of $\max(C \cdot K, 1)$ clients\;
			\ForEach{client $k \in S_t$ \textbf{in parallel}}{
				$w_{t+1}^{k} \leftarrow \texttt{ClientUpdate}(k, w_t)$\;
			}
			$w_{t+1} \leftarrow \sum_{k=1}^K \frac{n_k}{n} w_{t+1}^{k}$\;
		}
	}
	\Fn{\ClientUpdate{$k, w$}}{
		\For{each local epoch $i$ from $1$ to $E$}{
			batches $\leftarrow$ data $P_k$ split into batches of size $B$\;
			\For{each batch $b \in$ batches}{
				$w \leftarrow w - \eta \nabla \ell(w; b)$\;
			}
		}
		\KwRet $w$ to server\;
	}
\end{algorithm}

Auch wenn sich der Algorithmus in der Praxis bewährt hat\cite{hard:2018, ramaswamy:2020}, hat er Probleme bei nicht-gleichverteilten Daten. Um die Konvergenz unter solchen Bedingungen zu beschleunigen haben \textcite{karimireddy:2020} und \textcite{li:2020} abgewandelte Algorithmen entworfen.

\textcite{karimireddy:2020} entwickeln \texttt{SCAFFOLD}, einen Algorithmus, der versucht den \textit{client-drift} zu reduzieren, indem die Varianz der Updates durch \textit{control-variates} reduziert wird. \textit{client-drift} beschreibt das Verhalten, dass sich die einzelnen Clients jeweils zu ihrem lokalen Optimum bewegen und der Server zu dem Durchschnitt der Optima. Die Differenz zwischen dem Durchschnitt und dem tatsächlichen globalen Optimum muss bei \texttt{FedAvg} durch kleine Schritte gering gehalten werden wodurch die Konvergenz verlangsamt wird \parencite[p.4]{karimireddy:2020}.

\textcite{li:2020} erlauben den Clients in ihrem Algorithmus \texttt{FedProx} weniger exakte Updates zu erzeugen und fügen der lokalen Verlustfunktion einen \textit{proximal term} hinzu, der gewährleisten soll, dass die lokalen Parameter nicht zu stark von dem globalen Modell abweichen. Der Grund für das Zulassen von weniger exakten Updates ist, dass so weniger potente Clients weniger trainieren können um zu verhindern, dass ihre Ergebnisse aufgrund von Zeitüberschreitungen verworfen werden.

Ein weiteres Problem ist die Wahl geeigneter Hyperparameter für das Training. Da der eigentliche Trainingsprozess in Produktivumgebungen mehrere Tage\cite[p.5]{hard:2018} oder Wochen\cite[p.4]{ramaswamy:2020} dauern kann, ist es in der Regel nicht möglich viele Trainingsdurchläufe durchzuführen. In Verbindung mit Differential Privacy ist das Problem noch verschärft, da bei mehreren Trainingsdurchläufen auch die Privacy Kosten steigen. Damit gibt es dabei ebenfalls einen Trade-Off zwischen Netzwerk- und Ressourcennutzung und guten Hyperparametern. Auch \textcite[p.31]{kairouz:2021} plädieren für mehr Forschung in diesem Bereich und merken darüber hinaus an, dass die Anzahl der Hyperparameter im Federated Learning noch deutlich größer ist, als bei dem zentralisierten Training.

\section{Federated Learning with Differential Privacy}\label{sec:rw-fldp}
% evtl hier den ganzen Algorithmus näher beschreiben? -> mit Pseudo Code etc damit klar wird wo Rauschen hinzugefügt wird und wo geclippt wird
Zwar verhindert Federated Learning gegenüber zentralisierten Trainingsverfahren den direkten Austausch von Trainingsdaten, was intuitiv einen besseren Schutz der Privatheit nahelegt. Verschiedene Arbeiten \cite{wang:2019, geiping:2020, ma:2020} diskutieren Angriffe auf Federated Learning Systeme und zeigen, dass Modelle und Modellupdates auch im Federated Learning angegriffen und einzelne Trainingsbeispiele rekonstruiert werden können. In Verbindung mit Attacken auf klassisch trainierte Modelle zeigen diese Fälle die Notwendigkeit von verlässlichen Abwehrmechanismen wie Differential Privacy auch im Federated Learning. Die Wirksamkeit von Differential Privacy gegen Membership Inference Attacken im Federated Learning wird von \textcite{naseri:2022} noch einmal empirisch gezeigt, sowohl für die lokale als auch globale Anwendung von Differential Privacy.

\textcite{mcmahan:2018} entwickeln eine Version des \texttt{FedAvg}-Algorithmus, die Differential Privacy Garantien erfüllt, um lokale Sprachmodelle zu trainieren (\texttt{DP-FedAvg}). Darin begrenzen sie die $\ell_2$-Norm der Gradienten der Clients und können so die Sensitivität der Aggregation Updates abschätzen. Auf die gewichteten Summe der Client Updates wird entsprechend der Sensitivität Rauschen aus der Normalverteilung addiert. Außerdem wird nicht mehr eine feste Anzahl von Clients gezogen, sondern jeder Client mit einer festen Wahrscheinlichkeit gezogen. Dies ist ein wichtiges Detail, um genauere Abschätzungen des benötigten Privacy Budgets machen zu können \cite{wang:2020}. Sie zeigen, dass sie mit mehr Rechenleistung auch unter Einhaltung von Privacy-Budgets die gleichen Genauigkeiten erzielen können wie ohne Privacy-Garantien. Sie merken an, dass die benötigte Rechenleistung verringert werden kann, indem die Modelle nicht zufällig initialisiert werden, sondern auf öffentlichen Daten vortrainiert werden.

Einen ähnlichen Ansatz liefern \textcite{geyer:2017}, allerdings begrenzen sie die $\ell_2$-Norm nicht auf einen festen Wert, sondern berechnen jede Runde die Median-Norm der Updates aller in der Runde teilnehmenden Clients. Da sie auf dem Server berechnet werden muss findet auch das Stutzen der Updates auf die diese Norm auf dem Server statt. 

\textcite{andrew:2021} generalisieren dies in ihrer Arbeit und schlagen vor, statt festen Werten Quantile von Client Updates zu spezifizieren, bis zu denen gestutzt werden sollen. Dafür geben die Clients zusätzlich zu dem Update ein Bit zurück, das angibt ob ihr Update größer war als die \textit{Clipping Norm}. Basierend darauf berechnet der Server den Anteil der Clients, deren Updates gestutzt wurden. Dies geschieht ebenfalls durch einen DP-Mechanismus. Die \textit{Clipping Norm} für die nächste Runde wird dementsprechend mit einer spezifizierten \textit{Learning Rate} angepasst. Die richtige \textit{Clipping Norm} wird also auch trainiert. Sie können in ihrer Arbeit zeigen, dass das für die Berechnung der \textit{Clipping Norm} benötigte Budget häufig sehr klein ist, die dynamische Norm aber Vorteile im Training mit sich bringt.

\textcite{noble:2023} verweisen auf das Problem von heterogenen Daten im Federated Learning. Sie entwickeln basierend auf \texttt{SCAFFOLD} einen Algorithmus, \texttt{DP-SCAFFOLD}. Darüber hinaus analysieren sie dessen Konvergenz theoretisch und vergleichen ihn in Experimenten mit \texttt{DP-FedAvg}. Ihr Algorithmus erfüllt lokale Differential Privacy Garantien, schützt also die Updates der Clients auch gegenüber dem aggregierenden Server. Dieser ist laut ihrem Angreifermodell "honest-but-curious". Allerdings betrachten sie nur \textit{record-level} DP, geben also keine Privacy Garantien für ganze Clients, was eine schwächere Definition ist als \textit{user-level} DP \cite[p.44]{kairouz:2021}.

Sie evaluieren ihren Algorithmus auf dem \texttt{EMNIST}-Datensatz und auf synthetischen Daten, bei denen die Verzerrung der Datenverteilungen zwischen den Clients gesteuert werden kann. Bei \texttt{MNIST} weisen sie einen Teil der Daten zufällig zu und den übrigen Teil sortiert nach der Klasse, um Datenheterogenität zu simulieren. Die Ergebnisse zeigen, dass \texttt{DP-SCAFFOLD} bei gleichverteilten Daten zwar eine ein wenig schlechtere Performanz hat als \texttt{DP-FedAvg}, allerdings deutlich robuster ist, wenn die Daten zwischen den einzelnen Clients heterogener werden.

\textcite{ramaswamy:2020} nutzen den \texttt{DP-FedAvg} als Grundlage, um ein \textit{Next-Word-Prediction} Modell in einer Produktivumgebung zu trainieren. Im Vorfeld des Trainings führen sie ein Hyperparameter Tuning auf einem öffentlichen Datensatz durch, um Privacy-Kosten zu vermeiden. Sie zeigen, dass sie die Nützlichkeit gegenüber dem vorher genutzten Sprachmodell steigern können. Allerdings machen sie Abstriche bei dem formalen Nachweis des genutzten Privacy-Budgets, da das zufällige Sampling der Clients nicht eingehalten werden kann, welches nötig ist um das Budget gemäß dem Theorem von \textcite{wang:2020} abzuschätzen. Stattdessen weisen sie empirisch nach, dass ihre Anwendung von Differential Privacy gegen das ungewollte Einprägen von exakten Trainingsdaten im Modell hilft.


\section{Federated Learning with Individualized Differential Privacy}\label{sec:rw-flidp}

Wie bei zentralen Lernverfahren, gibt es auch im Federated Learning mit Differential Privacy einen Trade-Off zwischen Nützlichkeit und Privatheit der Modelle. Auch hier ist die Individualisierung von Privacy Budgets eine Möglichkeit, um die Nützlichkeit zu verbessern und trotzdem die Privacy Anforderungen der einzelnen Nutzer einzuhalten.

\textcite{aldaghri:2023} stellen in ihrer Arbeit einen Algorithmus zum Training mit individuellen Privacy Budgets im Federated Learning vor. Im ersten Schritt leiten sie einen Algorithmus für lineare Regressionen her, im zweiten einen darauf basierenden generellen Algorithmus. Ihr Algorithmus arbeitet mit individuellen \textit{noise multipliers} und einer uniformen \textit{sampling rate}. Anders als \citeauthor{boenisch:2023} evaluieren sie ihren Algorithmus nur mit zwei Abstufungen von Privacy Budgets, nämlich privaten und nicht privaten Clients.

\textcite{yang:2021} entwickeln mit \texttt{PLU-FedOA} einen Algorithmus, der lokale Differential Privacy für individuelle Privacy Budgets umsetzt. Dafür fügt jeder Client lokal basierend auf seinem Privacy-Budget und dem Median seines Gradienten Rauschen aus der Laplace-Verteilung zu seinem Gradienten hinzu. Nachdem die Gradienten an den Server geschickt werden aggregiert dieser sie, basierend auf einer gewichteten Summe, deren Gewichte anhand der Varianz der Updates bestimmt werden. Sie evaluieren ihren Algorithmus auf MNIST mit drei verschiedenen Verteilungen von Privacy-Budgets ($\epsilon \in [1;9]$). Je nach Verteilung erzielen sie Genauigkeiten von $78\%$ bis $94\%$. Nachdem sie die Anzahl der Clients in dem Datensatz erhöhen kommen sie auf $98\%$ Genauigkeit, allerdings nutzen sie dann über 150 Runden 600 Clients pro Runde.

\textcite{liu:2021} fokussieren sich in ihrer Arbeit auf ein \textit{cross-silo}-Szenario. In ihrem Algorithmus \texttt{PFA} versuchen sie anstelle verrauschter Updates die \glqq{}richtigen\grqq{} Informationen privater Clients zu identifizieren. Dazu extrahieren sie den wichtigsten Unterraum der Updates öffentlicher Clients und projizieren die Updates privater Clients auf diesen. So reduzieren sie ebenfalls die Dimensionalität der Updates. In einer Erweiterung ihres Algorithmus (\texttt{PFA+}) erlauben sie den privateren Clients die Projektion ihrer Modellupdates an den Server zu schicken und können so den Kommunikationsaufwand senken. Die Clients selbst trainieren auf ihren Daten mithilfe des \texttt{DP-SGD}\cite{abadi:2016}. Zusätzlich nutzen sie einen Accountant, basierend auf dem \texttt{Moments Accountant}, um aus dem Training auszusteigen, sobald ihr Privacy-Budget erreicht ist.

Sie evaluieren ihren Algorithmus auf verschiedenen Datensätzen, unter anderem auf MNIST und CIFAR10. Da sie sich auf das \textit{cross-silo} Szenario fokussieren, teilen sie die Daten auf nur $20$ bis $50$ Clients auf.

\textcite{shen:2023} entwickeln in ihrer Arbeit ebenfalls einen Algorithmus (\texttt{PLDP-FL}), der lokale DP bei individuellen Privacy Budgets erfüllt. Ihre Definition von Privacy ist aus \textcite{chen:2016}. Bei dieser wird ein Bereich $\tau$ eingeführt, für dessen Werte dann $\epsilon$-DP erfüllt wird. Sie evaluieren ihren Ansatz und vergleichen ihn mit \texttt{FedAvg} \parencite{mcmahan:2016} als oberer Schranke und \texttt{PLU-FedOA} aus \textcite{yang:2021}. Hier liegt die Performance ihres Algorithmus unter, aber nahe an \texttt{FedAvg} und über \texttt{PLU-FedOA}. 