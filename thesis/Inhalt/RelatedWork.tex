\chapter{Related work}

In diesem Kapitel beschreibe ich den Stand der Forschung in den für meine Arbeit relevanten Forschungsgebieten. Da sie mehrere Problemfelder umfasst, werde ich das Kapitel im folgenden in drei Teile unterteilen. In \autoref{sec:rw-fl} beschreibe ich den Stand der Forschung in Bezug auf Fragen des Federated Learning, beispielsweise wie mit unterschiedlichen Verteilungen in den Trainingsdatensätzen umgegangen werden kann. In \autoref{sec:rw-idp-ml}
In \autoref{sec:rw-flidp}

\section{Individualized Differential Privacy in Machine Learning}\label{sec:rw-idp-ml}

\textcite{boenisch:2023} untersuchen in ihrer Arbeit das Training mit individuellen Privacy Budgets. Sie stellen zwei Verfahren vor: \textbf{SAMPLE} und \textbf{SCALE}. \textbf{SAMPLE} ist basiert auf \textcite{jorgensen:2015} und erreicht die individuellen Budgets indem Datenpunkte im Training mit unterschiedlichen Wahrscheinlichkeiten gezogen werden. Allerdings werden die Wahrscheinlichkeiten anders berechnet als bei \textcite{jorgensen:2015}. \textbf{SCALE} ist inspiriert von \textcite{alaggan:2016}. Anders als bei deren \textit{Stretching Mechanism} werden allerdings nicht die Datenpunkte selbst skaliert, sondern das addierte Rauschen. 

Beide Ansätze werden mit verschiedenen Budgets und Verteilungen der Budgets evaluiert. Sie teilen die Trainingsdaten in drei Gruppen ein (niedriges, mittleres und hohe Privacy Budget). Darüber hinaus testen sie zwei Szenarien, bei denen sich die Größen der drei Gruppen unterscheiden. Die erste Verteilung lässt mehr Trainingspunkte mit hohem und mittleren Budget zu ($34\%$, $43\%$, $23\%$), die andere weniger ($54\%$, $37\%$, $9\%$). Wie bei \textcite{jorgensen:2015} sind die Ergebnisse im ersten Fall besser und werden im zweiten Fall schlechter. In der Evaluation schneidet \textbf{SAMPLE} in allen Experimenten bis auf einem etwas besser ab.

% PATE bzw Individualized PATE hinzunehmen?
Auch für PATE haben \textcite{boenisch:2023b} eine individualisierte Version entwickelt, IPATE. Sie erforschen zwei unterschiedliche Ansätze: \textit{upsampling} und \textit{weighting}. Der erste dupliziert Datenpunkte in Abhängigkeit von ihrem Privacy-Budget. Damit ist die Disjunktheit der Daten der einzelnen \textit{Teacher Models} nicht mehr gegeben. Mit \textit{weighting} gewichten sie die Stimmen der unterschiedlichen \textit{Teacher Models} unterschiedlich. Sie analysieren die Privacy-Kosten von PATE und IPATE und können mit dem individualisierten Ansatz deutlich mehr Trainingsdaten für das \textit{Student Model} klassifizieren bis die Privacy-Budgets aufgebraucht sind. Außerdem zeigen sie, dass die zusätzlichen Trainingsdaten einen positiven Effekt auf die Genauigkeit des \textit{Student Models} hat.

\section{Federated Learning}\label{sec:rw-fl}

Die Forschung zu Federated Learning, die abgesehen von Differential Privacy für meine Arbeit relevant ist, beschäftigt sich damit wie die Konvergenz von Federated Learning Algorithmen verbessert werden kann. Dabei geht es vor allem um die Behandlung von unterschiedlich verteilten Datensätzen, aber auch um den Ausfall von Clients und die Verringerung der Kommunikationskosten.

Die von \textcite{mcmahan:2016} vorgestellten Algorithmen \texttt{FedSGD} und vor allem \texttt{FedAvg} sind die de facto Standardalgorithmen für das Federated Learning. \textcite{karimireddy:2020} stellen einen weiteren Algorithmus vor, \texttt{SCAFFOLD}, der versucht den \textit{client-drift} zu reduzieren, indem die Varianz der Updates durch \textit{control-variates} reduziert wird. \textit{client-drift} beschreibt das Verhalten, dass sich die einzelnen Clients jeweils zu ihrem lokalen Optimum bewegen und der Server zu dem Durchschnitt der Optima. Die Differenz zwischen dem Durchschnitt und dem tatsächlichen globalen Optimum muss bei \texttt{FedAvg} durch kleine Schritte gering gehalten werden wodurch die Konvergenz verlangsamt wird \parencite[p.4]{karimireddy:2020}.

\begin{itemize}
	\item Hyperparameter Tuning
\end{itemize}

\section{Federated Learning with Individualized Differential Privacy}\label{sec:rw-flidp}

\begin{itemize}
	\item Client-level vs record-level dp
\end{itemize}

\textcite{mcmahan:2018} entwickeln eine Version des FedAvg-Algorithmus, die Differential Privacy Garantien erfüllt, um lokale Language Modelle zu trainieren. Darin begrenzen sie die $\ell_2$-Norm der Gradienten der Clients und können so die Sensitivität der Aggregation der Clients abschätzen. Einen ähnlichen Ansatz liefern \textcite{geyer:2017}, allerdings berechnen sie die Sensitivität der einzelnen Gradienten basierend auf der Median-Norm.

\textcite{aldaghri:2023} stellen in ihrer Arbeit einen Algorithmus zum Training mit individuellen Privacy Budgets im Federated Learning vor. Im ersten Schritt leiten sie einen Algorithmus für lineare Regressionen her, im zweiten einen darauf basierenden generellen Algorithmus. Ihr Algorithmus arbeitet mit individuellen \textit{noise multipliers} und einer uniformen \textit{sampling rate}. Anders als \citeauthor{boenisch:2023} evaluieren sie ihren Algorithmus nur mit zwei Abstufungen von Privacy Budgets, nämlich privaten und nicht privaten Clients.

\textcite{yang:2021} unterscheiden in ihrer Arbeit zwischen Algorithmen mit globaler und lokaler DP im Federated Learning. Letztere schützen nicht nur das entstehende globale Modell, sondern auch die Gradientenupdates der Clients gegenüber dem aggregierenden Server. Sie entwickeln einen Algorithmus, der lokale DP für individuelle Privacy Budgets erfüllt.

\textcite{liu:2021} merken in ihrer Arbeit an, dass während des Trainings das Modell einen Bias bezüglich der Daten von weniger privaten Clients entwickeln kann. In ihrem Ansatz versuchen sie daher anstelle verrauschter Updates die "richtigen" Informationen privater Clients zu extrahieren. Dazu extrahieren sie den wichtigsten Unterraum der Updates öffentlicher Clients und projizieren die Updates privater Clients auf diesen.

\textcite{shen:2023} entwickeln in ihrer Arbeit ebenfalls einen Algorithmus, der lokale DP bei individuellen Privacy Budgets erfüllt. Ihre Definition von Privacy ist aus \textcite{chen:2016}. Bei dieser wird ein Bereich $\tau$ eingeführt, für dessen Werte dann $\epsilon$-DP erfüllt wird. Sie evaluieren ihren Ansatz und vergleichen ihn mit FedAvg \parencite{mcmahan:2016} als oberer Schranke und PLU-FedOA aus \textcite{yang:2021}. Hier liegt die Performance ihres Algorithmus unter, aber nahe an FedAvg und über PLU-FedOA.

\textcite{noble:2023} verweisen auf das Problem von heterogenen Daten im Federated Learning. Sie entwickeln einen Algorithmus, der in ihren Experimenten FedAvg schlägt, wenn die Daten der Clients heterogener werden. Als Basis nutzen sie SCAFFOLD, einen Algorithmus, die Gradienten mit einer durchschnittlichen Richtung, in die alle Clients optimieren kombiniert. Als Angreifermodell nutzen sie den "honest-but-curious" Server (lokale DP-Garantien). 

\begin{itemize}
	\item Konvergenz des FL verbessern durch Algorithmen
	\begin{itemize}
		\item Varianz reduzieren (SCAFFOLD), Momente nutzen, ... (sehr viele Ansätze werden in \textcite[p.26ff]{kairouz:2021} erwähnt)
	\end{itemize}
	\item Ansätze für personalisierte Modelle (bei non-iid Datensätzen) \parencite[p.28ff]{kairouz:2021} bzw. auch die Idee Globale Modelle durch Kontext anzureichern um bessere Vorhersagen für einzelne Nutzer zu bekommen
	\item Übertragung von non-FL Verfahren auf FL und die Probleme die dabei entstehen (z.B. beim Hyperparametertuning) (p.30ff)
	\item Kompression / Effizienzsteigerung von FL (p.32ff.)
	\item sehr interessanter Absatz zu Kompatibilität von DP und Kompressionstechniken (p.33)
	\item DP in FL (p. 44ff), vor allem die Referenzen zu Hybrid DP tun das was ich machen will aber anders
	\item Kapitel 4.3 (p.48 ff) für mein Szenario mit einem vertrauenswürdigen Server
	\item p.50 hat vieles was bei mir in Future Work kann
	\item p.54 hat ganz klaren Vergleich zwischen Local und Central DP!
\end{itemize}