\chapter{Introduction}
%\begin{itemize}
%	\item Angriffe auf FL belegen
%\end{itemize}

With the increasing adaptation of AI in many everyday areas of life and the interest in representative training data, the question of data security and the protection of the privacy of people who generate training data for models is becoming increasingly important. Decentralised training algorithms that prevent data generated on smartphones, for example, from having to leave the device and mathematically verifiable privacy guarantees are becoming increasingly important in this area.

Differential privacy has proven to be a good measure for guaranteeing the privacy of individual users in a variety of data analyses. The idea is to modify query results in such a way that general trends are reflected in the result, but the influence of individual data points is limited. The concept is mathematically formalised and allows the level of privacy to be controlled via a parameter. It has also been used by large companies in the private sector for a number of years. Apple, Google and Uber in particular rely on differential privacy \parencite{apple:2017, erlingsson:2014, tezapsidis:2017} when analysing monitoring data. 

When training AI models on centralised data sets, the Differential Privacy Stochastic Gradient Descent (\texttt{DP-SGD}) from \textcite{abadi:2016} is an established and efficient method. Due to the formally provable privacy guarantees, this can lead to more trust in the use of data generated by humans. However, there is particularly sensitive data, such as that from smartphone keyboards, which a large proportion of users probably do not want to leave their end device.

To enable training without the need to centralise the training data, \textcite{mcmahan:2016} introduced \textit{Federated Learning}. Here, \glqq{}the learning task is solved by a loose federation of participating devices (called clients in this work) that are coordinated by a central server.\grqq{} \parencite[p.1]{mcmahan:2016} Even if this solves the problem of centralising training data, it does not yet guarantee the privacy of the training data, as attacks on model parameters are not prevented. To address this problem, modifications to the algorithm were presented that fulfil differential privacy guarantees for the clients \parencite{mcmahan:2018, geyer:2017}.

Even though differential privacy in the training of AI models protects the privacy of individuals and is indispensable against some attacks, it has a major disadvantage: there is a trade-off between the quality of the trained models and the privacy of the training data, higher privacy reduces accuracy. This is usually negligible to a certain extent, nevertheless there are ideas to make this trade-off more efficient for cases where it does matter. 

As different people generate different types of sensitive data and generally have different privacy requirements, there is research into individualising privacy guarantees. For example, people could choose whether they want a medium, high or very high level of privacy for their data. AI models could thus learn more from data with a lower privacy level during training and achieve better accuracy overall.

Federated learning also uses algorithms to utilise individual privacy guarantees. However, some of them require newly introduced parameters \parencite{shen:2023} or individualise noise multipliers \parencite{aldaghri:2023}, which was not optimal, at least in centralised training \parencite{boenisch:2023}. There, individualised sampling rates, i.e. the probability of using a data point in a training step, delivered slightly better results.

In this work, I provide an approach to use individualised sampling rates in federated learning to ensure individual privacy guarantees for clients. In addition, I also address challenges in the application of federated learning for training the models. These include, for example, the distribution of the training data to the clients and the additional computational effort. I will only mention and describe other problems that occur in practice, as I can only simulate federated learning. However, they should definitely be taken into account. These include the heterogeneity of the devices on which training takes place and the availability or failure of clients during training.

In \autoref{chap:fundamentals} I describe the basics of differential privacy and federated learning. In \autoref{chap:related-work} I describe work on individualised differential privacy and differential privacy in federated learning. \autoref{chap:methods} describes my algorithm and shows how it differs from existing work. The experiments I conducted and their results are described in \autoref{chap:experiments} and \autoref{chap:results} respectively.