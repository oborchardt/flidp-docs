% belegen, dass FL alleine nicht reicht
\chapter{Introduction}

Mit der zunehmenden Adaption von KI in vielen alltäglichen Bereichen des Lebens und dem Interesse an repräsentativen Trainingsdaten gewinnt die Frage nach Datensicherheit und dem Schutz der Privatheit der Menschen, die Trainingsdaten für Modelle generieren, immer weiter an Bedeutung. Gerade das Training erfordert Ansätze, die komplexer sind als die klassische Optimierung der Modelle auf zentralen Datensätzen.

Differential Privacy hat sich als gute Maßnahme erwiesen, um die Privatheit von einzelnen Nutzern bei einer Vielzahl von Datenauswertungen zu gewährleisten. Auch beim Training von Modellen auf zentralen Datensätzen gibt es mit dem \texttt{DP-SGD} von \textcite{abadi:2016} ein etabliertes und effizient anwendbares Verfahren. Durch die formal beweisbaren Garantien, kann dies zu mehr Vertrauen bei der Nutzung von von Menschen generierten Daten geben.

Um das Training zu ermöglichen ohne eine Zentralisierung der Trainingsdaten erforderlich zu machen, haben \textcite{mcmahan:2016} das \textit{Federated Learning} vorgestellt. Dabei wird \glqq{]die Lernaufgabe durch eine lose Föderation von teilnehmenden Geräten (die wir als Clients bezeichnen) gelöst, die von einem zentralen Server koordiniert werden.\grqq{} \parencite[p.1]{mcmahan:2016} Auch wenn damit das Problem der Zentralisierung von Trainingsdaten gelöst werden kann, ist damit noch nicht die Privatheit der Trainingsdaten gewährleistet, denn Angriffe auf Modellparameter werden nicht verhindert. Um dem Problem zu begegnen wurde ein Algorithmus vorgestellt, der für die Clients Differential Privacy Garantien erfüllt \parencite{mcmahan:2018}.

Die Anwendung von Differential Privacy beim Training von KI-Modellen verschlechtert die Genauigkeit der Modelle. Es gibt also einen Trade-Off zwischen der Genauigkeit der Modelle und dem Grad der Privatheit der Trainingsdaten, der gewährleistet werden soll. Ein höherer Grad an Privatheit verschlechtert die Genauigkeit. Das ist in der Regel bis zu einem Gewissen Grad vernachlässigbar. Da unterschiedliche Menschen unterschiedlich brisante Daten generieren und im Allgemeinen auch unterschiedliche Anforderungen an ihre Privatheit haben, gibt es Forschung dazu, die Privatheitsgarantien zu individualisieren. So könnten Personen wählen ob sie beispielsweise ein mittleres, hohes oder sehr hohes Privatheitsniveau für ihre Daten möchten. KI-Modelle würden so während ihres Trainings unterschiedlich viel aus den Daten der verschiedenen Niveaus lernen, und insgesamt eine bessere Genauigkeit erreichen.

Auch im Federated Learning gibt es Algorithmen, um individuelle Privatheitsgarantien zu nutzen. Allerdings erfordern sie teilweise neu eingeführte Parameter \parencite{shen:2023} oder individualisieren Noise Multiplier \parencite{aldaghri:2023}, was zumindest im zentralisierten Training nicht optimal war \parencite{boenisch:2023}. Dort haben individualisierte Sampling Raten, also die Wahrscheinlichkeit, einen Datenpunkt in einem Trainingsschritt zu nutzen, etwas bessere Ergebnisse geliefert.

In meiner Arbeit liefere ich einen Ansatz, individualisierte Sampling Raten im Federated Learning zu nutzen, um individuelle Privatheitsgarantien für Clients zu gewährleisten. Darüber hinaus gehe ich auch auf Herausforderungen bei der Anwendung von Federated Learning für das Training der Modelle ein. Dazu zählen beispielsweise die Verteilung der Trainingsdaten auf den Clients und der zusätzliche Berechnungsaufwand. Weitere Probleme, die in der Praxis auftreten, werde ich nur erwähnen und beschreiben, da ich das Federated Learning nur simulieren kann. Sie sollten aber unbedingt beachtet werden. Dazu zählen die Heterogenität der Geräte auf denen trainiert wird und die Verfügbarkeit der Clients für das Training.