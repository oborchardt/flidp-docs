\chapter{Introduction}
\begin{itemize}
	\item Angriffe auf FL belegen
\end{itemize}

Mit der zunehmenden Adaption von KI in vielen alltäglichen Bereichen des Lebens und dem Interesse an repräsentativen Trainingsdaten gewinnt die Frage nach Datensicherheit und dem Schutz der Privatheit der Menschen, die Trainingsdaten für Modelle generieren, immer weiter an Bedeutung.In dem Bereich gewinnen dezentrale Trainingsalgorithmen, die verhindern, dass zum Beispiel auf Smartphones generierte Daten das Gerät verlassen müssen, und mathematisch beweisbare Privatheitsgarantien an Bedeutung.

Differential Privacy hat sich als gute Maßnahme erwiesen, um die Privatheit von einzelnen Nutzern bei einer Vielzahl von Datenauswertungen zu gewährleisten. Die Idee ist, Anfrageergebnisse so zu verändern, dass allgemeine Trends im Ergebnis abgebildet sind, der Einfluss einzelner Datenpunkte jedoch begrenzt wird. Das Konzept ist mathematisch formalisiert und lässt es zu, das Niveau der Privatheit über einen Parameter zu steuern. Es wird seit einigen Jahren auch von großen Firmen in der Privatwirtschaft angewendet. Gerade bei der Auswertung von Monitoring Daten setzen Apple, Google und Uber auf Differential Privacy \parencite{apple:2017, erlingsson:2014, tezapsidis:2017}. 

Beim Training von KI-Modellen auf zentralen Datensätzen gibt es mit dem Differential Privacy Stochastic Gradient Descent (\texttt{DP-SGD}) von \textcite{abadi:2016} ein etabliertes und effizient anwendbares Verfahren. Durch die formal beweisbaren Privatheitsgarantien, kann dies zu mehr Vertrauen bei der Nutzung von Daten geben, die von Menschen generiert werden. Allerdings gibt es besonders sensible Daten, wie zum Beispiel die von Smartphone Tastaturen, von denen ein großer Teil der Nutzer vermutlich nicht will, dass sie ihr Endgerät verlassen.

%Quellen für angriffe auf FL angeben
Um das Training zu ermöglichen ohne eine Zentralisierung der Trainingsdaten erforderlich zu machen, haben \textcite{mcmahan:2016} das \textit{Federated Learning} vorgestellt. Dabei wird \glqq{]die Lernaufgabe durch eine lose Föderation von teilnehmenden Geräten (die wir als Clients bezeichnen) gelöst, die von einem zentralen Server koordiniert werden.\grqq{} \parencite[p.1]{mcmahan:2016} Auch wenn damit das Problem der Zentralisierung von Trainingsdaten gelöst werden kann, ist damit noch nicht die Privatheit der Trainingsdaten gewährleistet, denn Angriffe auf Modellparameter werden nicht verhindert. Um dem Problem zu begegnen wurden Modifikationen des Algorithmus vorgestellt, die für die Clients Differential Privacy Garantien erfüllen \parencite{mcmahan:2018, geyer:2017}.

Auch wenn Differential Privacy beim Training von KI-Modellen die Privatheit der Individuen schützt und gegen einige Angriffe unerlässlich ist, hat sie einen großen Nachteil: Es gibt einen Trade-Off zwischen der Güte der trainierten Modelle und der Privatheit der Trainingsdaten, eine höhere Privatheit verringert die Genauigkeit. Das ist in der Regel bis zu einem Gewissen Grad vernachlässigbar, dennoch gibt es Ideen um diesen Trade-Off effizienter zu machen. 

Da unterschiedliche Menschen unterschiedlich brisante Daten generieren und im Allgemeinen auch verschiedene Anforderungen an ihre Privatheit haben, gibt es Forschung dazu, die Privatheitsgarantien zu individualisieren. So könnten Personen wählen ob sie beispielsweise ein mittleres, hohes oder sehr hohes Privatheitsniveau für ihre Daten möchten. KI-Modelle könnten so während des Trainings mehr aus Daten mit einem kleineren Privatheitsniveau lernen und insgesamt eine bessere Genauigkeit erreichen.

Auch im Federated Learning gibt es Algorithmen, um individuelle Privatheitsgarantien zu nutzen. Allerdings erfordern sie teilweise neu eingeführte Parameter \parencite{shen:2023} oder individualisieren Noise Multiplier \parencite{aldaghri:2023}, was zumindest im zentralisierten Training nicht optimal war \parencite{boenisch:2023}. Dort haben individualisierte Sampling Rates, also die Wahrscheinlichkeit, einen Datenpunkt in einem Trainingsschritt zu nutzen, etwas bessere Ergebnisse geliefert.

In meiner Arbeit liefere ich einen Ansatz, individualisierte Sampling Rates im Federated Learning zu nutzen, um individuelle Privatheitsgarantien für Clients zu gewährleisten. Darüber hinaus gehe ich auch auf Herausforderungen bei der Anwendung von Federated Learning für das Training der Modelle ein. Dazu zählen beispielsweise die Verteilung der Trainingsdaten auf den Clients und der zusätzliche Berechnungsaufwand. Weitere Probleme, die in der Praxis auftreten, werde ich nur erwähnen und beschreiben, da ich das Federated Learning nur simulieren kann. Sie sollten aber unbedingt beachtet werden. Dazu zählen die Heterogenität der Geräte auf denen trainiert wird und die Verfügbarkeit bzw. das Ausfallen der Clients während des Trainings.

In \autoref{chap:fundamentals} beschreibe ich die Grundlagen von Differential Privacy und dem Federated Learning. In \autoref{chap:related-work} beschreibe ich Arbeiten zu individualisierter Differential Privacy und Differential Privacy im Federated Learning. \autoref{chap:methods} beschreibt meinen Algorithmus und zeigt, wie er sich von vorhandenen Arbeiten unterscheidet. Die Experimente, die ich durchgeführt habe und deren Ergebnisse werden in \autoref{chap:experiments} bzw. in \autoref{chap:results} beschrieben.