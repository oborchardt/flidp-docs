\chapter{Einleitung}
%\begin{itemize}
%	\item Angriffe auf FL belegen
%\end{itemize}

Mit der zunehmenden Adaption von künstlicher Intelligenz (KI) in vielen alltäglichen Bereichen des Lebens und dem Interesse an repräsentativen Trainingsdaten gewinnt die Frage nach Datensicherheit und dem Schutz der Privatheit der Personen, die Trainingsdaten für Modelle generieren, immer weiter an Bedeutung. In dem Bereich gewinnen dezentrale Trainingsalgorithmen, die verhindern, dass zum Beispiel auf Smartphones generierte Daten das Gerät verlassen müssen und mathematisch beweisbare Privatheitsgarantien an Bedeutung.

\textit{Differential Privacy} (DP) hat sich als gute Maßnahme erwiesen, um die Privatheit von einzelnen Nutzern bei einer Vielzahl von Datenauswertungen zu gewährleisten. Die Idee ist, Anfrageergebnisse so zu verändern, dass allgemeine Trends im Ergebnis abgebildet sind, der Einfluss einzelner Datenpunkte jedoch begrenzt wird. Das Konzept ist mathematisch formalisiert und ermöglicht es, das Niveau der Privatheit über einen Parameter zu steuern. Es wird seit einigen Jahren von großen Firmen angewendet. Gerade bei der Auswertung von Monitoring Daten setzen Apple, Google und Uber auf \textit{Differential Privacy} \parencite{apple:2017, erlingsson:2014, tezapsidis:2017}. 

Beim Training von KI-Modellen auf zentralen Datensätzen gibt es mit dem \textit{Differential Privacy Stochastic Gradient Descent} (\texttt{DP-SGD}) von \textcite{abadi:2016} ein etabliertes und effizient anwendbares Verfahren. Durch die formal beweisbaren Privatheitsgarantien, kann dies zu mehr Vertrauen bei der Nutzung von Daten geben, die von Menschen generiert werden. Allerdings gibt es besonders sensible Daten, wie zum Beispiel die von Smartphone Tastaturen, von denen ein großer Teil der Nutzer vermutlich nicht will, dass sie ihr Endgerät verlassen.

%Quellen für angriffe auf FL angeben
Um das Training von KI-Modellen zu ermöglichen, ohne eine zentrale Speicherung der Trainingsdaten erforderlich zu machen, haben \textcite{mcmahan:2016} das \textit{Federated Learning} vorgestellt. Dabei wird \glqq{}die Lernaufgabe durch eine lose Föderation von teilnehmenden Geräten (die wir als Clients bezeichnen) gelöst, die von einem zentralen Server koordiniert werden.\grqq{} \parencite[p.1]{mcmahan:2016} Auch wenn damit das Problem der Zentralisierung von Trainingsdaten behoben werden kann, ist damit noch nicht die Privatheit der Trainingsdaten gewährleistet, denn Angriffe auf Modellparameter werden nicht verhindert. Um dieser Frage zu begegnen, wurden Modifikationen des Algorithmus vorgestellt, die DP-Garantien für die Clients erfüllen \parencite{mcmahan:2018, geyer:2017}.

Auch wenn \textit{Differential Privacy} beim Training von KI-Modellen die Privatheit der Individuen schützt und gegen einige Angriffe unerlässlich ist, hat sie einen großen Nachteil: es gibt einen Trade-Off zwischen der Güte der trainierten Modelle und der Privatheit der Trainingsdaten. Eine höhere Privatheit der Daten verringert die Genauigkeit der Modelle. Das ist in der Regel bis zu einem gewissen Grad vernachlässigbar, dennoch gibt es Ideen um diesen Trade-Off effizienter zu gestalten. 

Da unterschiedliche Menschen unterschiedlich brisante Daten generieren und im Allgemeinen auch verschiedene Anforderungen an ihre Privatheit haben, gibt es Forschungen dazu, die Privatheitsgarantien zu individualisieren. So könnten Personen wählen, ob sie beispielsweise ein mittleres, hohes oder sehr hohes Privatheitsniveau für ihre Daten möchten. KI-Modelle könnten während des Trainings aus Daten mit einem kleineren Privatheitsniveau mehr lernen und damit eine bessere Genauigkeit erreichen.

Auch im \textit{Federated Learning} gibt es Algorithmen, um individuelle Privatheitsgarantien zu nutzen. Allerdings erfordern sie teilweise neu eingeführte Parameter \parencite{shen:2023} oder individualisieren \textit{Noise Multiplier} \parencite{aldaghri:2023}, was sich zumindest im zentralisierten Training als nicht optimal herausgestellt hat \parencite{boenisch:2023}. Dort haben individualisierte Sampling-Raten, also die Wahrscheinlichkeit, einen Datenpunkt in einem Trainingsschritt zu nutzen, etwas bessere Ergebnisse geliefert.

In dieser Arbeit entwickle und evaluiere ich einen weiteren Ansatz, um individualisierte Sampling-Raten im \textit{Federated Learning} zu nutzen, und so individuelle Privatheitsgarantien für Clients zu gewährleisten. Darüber hinaus gehe ich auch auf Herausforderungen bei der Anwendung von \textit{Federated Learning} für das Training der Modelle ein. Dazu zählen beispielsweise die Verteilung der Trainingsdaten auf den Clients und der zusätzliche Berechnungsaufwand. Weitere Probleme, die in der Praxis auftreten, werde ich nur erwähnen und beschreiben, da ich das \textit{Federated Learning} nur simulieren kann. Sie sollten aber unbedingt beachtet werden. Dazu zählen die Heterogenität der Geräte auf denen trainiert wird und die Verfügbarkeit bzw. das Ausfallen der Clients während des Trainings.

In \autoref{chap:fundamentals} werden die Grundlagen von \textit{Differential Privacy} und \textit{Federated Learning} behandelt. \autoref{chap:related-work} widmet sich Arbeiten zu individualisierter \textit{Differential Privacy} sowie deren Anwendung im \textit{Federated Learning}. In \autoref{chap:methods} erläutere ich meinen Algorithmus und zeige dessen Unterschiede zu bestehenden Ansätzen auf. Die durchgeführten Experimente und die erzielten Ergebnisse sind in \autoref{chap:experiments} beziehungsweise \autoref{chap:results} dargestellt.