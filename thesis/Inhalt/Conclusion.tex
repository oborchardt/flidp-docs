\chapter{Zusammenfassung}

In dieser Arbeit wird ein Algorithmus entwickelt, um im Federated Learning individualisierte Privacy Garantien zu erfüllen. Die Umsetzung erfolgt durch individuelle Sampling Rates, was bereits im klassischen Machine Learning und in der Datenanalyse ein vielversprechendes Verfahren ist.

Die Bedeutung von Differential Privacy im Machine Learning wird beleuchtet und basierend darauf wird begründet, warum sie auch im Federated Learning wichtig ist. So kann Federated Learning bestimmte Angriffe auf die trainierten Modelle nicht ausschließen. Der \textit{Privacy-Utility Tradeoff} wird beschrieben und davon ausgehend auch die Notwendigkeit, Algorithmen mit Differential Privacy für diesen Tradeoff zu optimieren. 

Individuelle Privacy Budgets können dazu einen Beitrag leisten, denn erst durch sie können Nutzerpräferenzen gut in privaten Algorithmen abgebildet werden. Durch verschiedene Studien ist belegt, dass diese Präferenzen bei der Privatheit der eigenen Daten sehr heterogen sind. Algorithmen, die individuelle Privacy Budgets nutzen, können daher Parteien, die KI-Modelle trainieren, die Möglichkeit geben, gute Modelle zu trainieren und trotzdem die Privacy-Anforderungen der Nutzer zu erfüllen. Damit kann auch auf ihrer Seite die Akzeptanz für das private Training erhöht werden und damit die Verbreitung privater Algorithmen gestärkt werden.

Der Algorithmus in dieser Arbeit gibt vor allem den Anstoß, individualisierte Differential Privacy durch individuelle Sampling Rates umzusetzen. Andere Algorithmen setzen dies durch individuelle Noise Multiplier oder zusätzliche Parameter um.

In Experimenten wird der Nutzen für die Modellgenauigkeit gezeigt. Es zeigt sich, dass die individuellen Budgets vor allem in Fällen, in denen ein Modell mit strengen einheitlichen Budgets nicht konvergiert, Vorteile mit sich bringen kann. Das Training mit weniger strengen einheitlichen Budgets würde in solchen Fällen die Privatheit einiger Nutzer verletzen.

Außerdem zeigen die Experimente, dass \texttt{FedAvg} als weit verbreiteter Algorithmus im Federated Learning mit heterogenen Verteilungen der Trainingsdaten große Probleme hat. Das deckt sich mit weiterer Forschung zu dem Thema und hat die Entwicklung speziellerer Algorithmen für heterogen verteilte Daten motiviert.

Auch die Abhängigkeit der Privacy Budgets von den Trainingsdaten wird thematisiert. So zeigen die Ergebnisse, dass Modelle bei schwierigeren Datensätzen mit kleinen Privacy Budgets kaum etwas lernen können, währen die gleichen Budgets bei einem einfacheren Datensatz ausreichend sind.

Abgesehen von der Implementierung des Algorithmus und den Experimenten werden Probleme des Federated Learning in Produktivumgebungen thematisiert. Dazu zählen die Schwierigkeit Hyperparameter zu optimieren, der erhöhte Rechenaufwand und die Unzuverlässigkeit von am Training partizipierenden Clients.

Insgesamt bleiben sowohl an Federated Learning als auch Differential Privacy zwei wichtige Forschungsrichtungen um die Privatheit von Nutzern auch in einer Zeit zu schützen, in der große, mit vielen Daten trainierte KI-Modelle im Alltag vieler Menschen angekommen sind. Sie können einen Kompromiss zwischen den Interessen der Nutzer und denen großer Unternehmen bilden. Wichtig ist, dass beide Verfahren für Nutzer transparent sind und für KI-Forscher gut nutzbar sind. Individuelle Privacy Budgets, die über Sampling Rates umgesetzt werden sind eine Möglichkeit dafür und sind ein Verfahren, dass gut in bestehende Algorithmen integrierbar ist.