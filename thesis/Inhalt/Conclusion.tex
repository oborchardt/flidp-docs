\chapter{Zusammenfassung}

In dieser Arbeit wird ein Algorithmus entwickelt, um im \textit{Federated Learning} individualisierte Privatheitsgarantien zu erfüllen. Die Umsetzung erfolgt durch individuelle Sampling-Raten. Dies ist bereits im zentralen Training mit \textit{Differential Privacy} und in der Datenanalyse ein vielversprechendes Verfahren.

Die Bedeutung von \textit{Differential Privacy} im Machine Learning wird beleuchtet und basierend darauf wird begründet, warum sie auch im \textit{Federated Learning} wichtig ist. So kann \textit{Federated Learning} bestimmte Angriffe auf die trainierten Modelle nicht ausschließen. Der \textit{Privacy-Utility Tradeoff} wird beschrieben und davon ausgehend auch die Notwendigkeit, Algorithmen mit \textit{Differential Privacy} für diesen Tradeoff zu optimieren. 

Algorithmen mit individuellen Privacy-Budgets können für diese Optimierung einen Beitrag leisten, denn erst durch sie können individuelle Nutzerpräferenzen im Training abgebildet werden. Durch verschiedene Studien ist belegt, dass die Präferenzen bezüglich der Privatheit der eigenen Daten sehr heterogen sind. Wenn die Präferenzen optimal ausgenutzt werden, können privat trainierte Modelle bessere Ergebnisse erzielen und trotzdem die Privacy-Anforderungen der einzelnen Nutzer erfüllen. Damit kann auch bei den Parteien, die die Modelle trainieren, die Akzeptanz für das private Training erhöht und damit die Verbreitung privater Algorithmen gestärkt werden.

Der Algorithmus in dieser Arbeit gibt vor allem den Anstoß, individualisierte \textit{Differential Privacy} durch individuelle Sampling-Raten umzusetzen. Andere Algorithmen setzen dies durch individuelle \textit{Noise Multiplier} oder zusätzliche Parameter um.

In Experimenten wird der Nutzen für die Modellgenauigkeit dargestellt. Es zeigt sich, dass die individuellen Budgets vor allem in Fällen Vorteile mit sich bringen, in denen ein Modell mit strengen einheitlichen Budgets nicht mehr konvergiert. Das Training mit weniger strengen einheitlichen Budgets würde hier die Privatheit einiger Nutzer verletzen.

Außerdem zeigen die Experimente, dass \texttt{FedAvg} als weitverbreiteter Algorithmus im \textit{Federated Learning} mit heterogenen Verteilungen der Trainingsdaten große Probleme hat. Das deckt sich mit weiterer Forschung zu dem Thema und hat die Entwicklung speziellerer Algorithmen für heterogen verteilte Daten motiviert.

Auch die Abhängigkeit der Privacy-Budgets von den Trainingsdaten wird thematisiert. So zeigen die Ergebnisse, dass Modelle bei schwierigeren Datensätzen mit kleinen Privacy-Budgets kaum etwas lernen können, während die gleichen Budgets bei einem einfacheren Datensatz ausreichend sind.

Abgesehen von der Implementierung des Algorithmus und den Experimenten werden Probleme des \textit{Federated Learning} in Produktivumgebungen thematisiert. Dazu zählen die Schwierigkeit Hyperparameter zu optimieren, der erhöhte Rechenaufwand und die Unzuverlässigkeit von am Training partizipierenden Clients.

Insgesamt bleiben sowohl \textit{Federated Learning} als auch \textit{Differential Privacy} zwei wichtige Forschungsrichtungen, um die Privatheit von Nutzern auch in einer Zeit zu schützen, in der große, mit vielen Daten trainierte KI-Modelle im Alltag vieler Menschen angekommen sind. Sie können einen Kompromiss zwischen den Interessen der Nutzer und denen großer Unternehmen bilden. Wichtig ist, dass beide Verfahren für Nutzer transparent und für KI-Forscher gut nutzbar sind. Individuelle Privacy-Budgets, die über Sampling-Raten umgesetzt werden, stellen dafür eine Möglichkeit dar und sind gut in bestehende Algorithmen integrierbar.