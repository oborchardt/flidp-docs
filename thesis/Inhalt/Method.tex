\chapter{Methodik}\label{chap:methods}

%\begin{itemize}
%	\item (argumentieren, warum ich rdp Accountant nutzen kann, also auf \cite[p.26]{boenisch:2023} verweisen)
%	\item auf die Schätzer eingehen, bzw. darauf, dass sie in der TF Implementierung keine Rolle spielen(?)
%	\item Effekte von Hyperparametern erklären? (mehr Runden mehr Privacy Loss etc)
%	\item Effekt des Noise Multipliers im Detail erklären
%	\item Algorithmus von DP-FedAvg (evtl mit Adaptive Clipping?) bzw direkt ganzen Algorithmus von mir
	
%	\item High Level Einordnung -> Beispiele werden durch Clients ersetzt, was ist der Unterschied zu Related work
%	\item In der Methode noch den Ablauf der Experimente High Level beschreiben -> kann sich ein bisschen mit Experimente wiederholen
%	\item Einordnung in Research Kosmos und was ich damit in Experimenten mache
%	\item General Structure zu Aufbau des Algorithmus -> Änderungen beschreiben auf Related Work verweisen
%\end{itemize}

% Methodik beschreiben
% evtl hier auch Erwartungen beschreiben, in Bezug auf die 3 Punkte aus der Conclusion von wei:2020
In meiner Arbeit entwickle ich einen Algorithmus, der Machine Learning Modelle mit individualisierten Privacy Garantien im Kontext des Federated Learning trainiert. Dieser soll individualisierte Differential Privacy durch individuelle Sampling Raten realisieren, ähnlich wie es \textcite{jorgensen:2015, boenisch:2023} getan haben.

Da lokale Differential Privacy mit einer verminderten Nützlichkeit einhergeht, setze ich die Differential Privacy im globalen Kontext um. Das heißt, dass ich die Privatheit des trainierten Modells, nicht aber die Zwischenergebnisse, die der Server sieht, gewährleisten kann.

In \autoref{chap:experiments} beschreibe ich, wie ich ihn empirisch auf Benchmark-Datensätzen evaluiere. Dabei simuliere ich den Effekt verschiedener Verteilungen von Privacy-Budgets auf verschiedenen Datensätzen. Außerdem baue ich in die Datensätze verschiedene Datenverzerrungen ein und vergleiche sie mit gleichverteilten Versionen der Datensätze. In \autoref{chap:results} präsentiere ich die Ergebnisse.

Mein Algorithmus grenzt sich dabei von Arbeiten mit dem gleichen Ziel wie beispielsweise \textcite{aldaghri:2023} vor allem durch die Evaluierung mit verschiedenen Privacy-Setups, realistischen Datensätzen und der Nutzung von Sampling Rates um individuelle Differential Privacy zu erreichen ab.

\section{Allgemeine Struktur}
Mein Algorithmus lässt sich in zwei Schritte aufteilen. Der erste umfasst die Berechnung von einem Noise Multiplier und den Sampling Rates für einzelne Clients. Der Noise Multiplier ist ein Faktor, der mit der Sensitivität multipliziert wird, um die Standardabweichung der Normalverteilung, aus der das Rauschen gezogen wird zu berechnen. Wenn er größer wird, wird auch das Rauschen größer. Das Verfahren dafür beschreibe ich in \autoref{sec:idp-sampling-rates}. 

Die Berechnung dieser Faktoren basiert auf dem \texttt{SAMPLE}-Algorithmus von \textcite{boenisch:2023}, allerdings sind die Parameter dafür an das Federated Learning Szenario angepasst. Darin entsprechen die Epochen den Runden und die Gradienten einzelner Trainingsbeispiele den Updates der Clients. So werden aus \textit{example-level} Garantien \textit{user-level} Garantien der Differential Privacy, wie in \autoref{sec:pm-in-fl} beschrieben.

Der zweite Schritt ist die Durchführung des eigentlichen Trainings mit diesen vorher berechneten Parametern. Dafür nutze ich \texttt{DP-FedAvg} mit Adaptive Clipping \cite{andrew:2021}, allerdings ziehe ich die Clients mit den vorher berechneten individuellen Sampling Rates. Anders als bei \citeauthor{aldaghri:2023} haben alle Clients einen einheitlichen Noise Multiplier und individuelle Sampling Rates.

Mit meinem Algorithmus habe ich Experimente auf drei Benchmark Datensätzen durchgeführt, um den Effekt der individuellen Privacy-Budgets und heterogenen Datenverteilungen zu analysieren. Details dazu sind in \autoref{chap:experiments} beschrieben.

% Beweis von \cite{boenisch:2023} zu Sample beschreiben oder übernehmen (basiert auf RDP und Privacy Amplification through Subsampling)
% auf das Thema User-level DP eingehen?
\section{Individualisierte Differential Privacy durch individuelle Sampling Rates}\label{sec:idp-sampling-rates}

\textcite{boenisch:2023} konnten in ihrer Arbeit zeigen, dass das Training mit individuellen \textit{sampling rates}, die auf Grundlage der individuellen Privacy Budgets berechnet werden, gute Ergebnisse erzielen kann. Die Ergebnisse sind insbesondere besser, als die, die sie mit der individuellen Skalierung der \textit{Noise Multiplier} bzw. der \textit{clipping norms} erzielt haben, einem Ansatz der auch im Federated Learning vorgeschlagen wurde \cite{aldaghri:2023}. 

Ihr Algorithmus zur Berechnung der \textit{sampling rates} behält dabei den Erwartungswert der Anzahl der Datenpunkte, die gezogen werden, bei. Der Algorithmus ist in \autoref{alg:boenisch-sample} zu sehen.

\begin{algorithm}[tb]
	\caption{\texttt{GetSampleRates}, per-group Privacy Budget Algorithm from \cite[p.6]{boenisch:2023}}
	\label{alg:boenisch-sample}
	\KwIn{Per-group target privacy budgets $\{\epsilon_1, \dots, \epsilon_P\}$, target $\delta$, iterations $I$, number of total data points $N$, per-privacy group data points $\{|G_1|, \dots, |G_P|\}$}
	\KwOut{$\sigma_\text{sample}$, $\{q_1, \dots, q_P\}$}
	
	\SetKwProg{Fn}{Function}{:}{}
	\SetKwFunction{getSampleRate}{getSampleRate}
	
	\BlankLine
	$\sigma_\text{sample} \leftarrow \sigma$
	
	\ForEach{$p \in [P]$}{
		$q_p \leftarrow$ \getSampleRate{$\epsilon_p, \delta, \sigma_\text{sample}, I$}\;
	}
	\While{$q \not\approx \sum_{p=1}^{P} \frac{|G_p|}{N} q_p$}{
		$\sigma_\text{sample} \leftarrow s_i \sigma_\text{sample}$ \tcp*[l]{scaling factor: $s_i < 1$}
		\ForEach{$p \in [P]$}{
			$q_p \leftarrow$ \getSampleRate{$\epsilon_p, \delta, \sigma_\text{sample}, I$}\;
		}
	}
	\KwRet $\sigma_\text{sample}, \{q_1, \dots, q_P\}$\;
\end{algorithm}

Vor der Anwendung des Algorithmus müssen die einzelnen Datenpunkte nach ihrem Privacy Budget gruppiert werden. So entstehen $P$ Gruppen, wobei $P$ die Anzahl der verschiedenen Privacy-Budgets im Datensatz ist.

Die Ausgabe des Algorithmus ist ein Noise-Multiplier $\sigma_{\text{SAMPLE}}$, der für alle Datenpunkte einheitlich ist, und individuelle Sampling Rates $\{q_1, ..., q_P\}\}$, welche die Wahrscheinlichkeit beschreiben einen Datenpunkt in einem Mini-Batch zu ziehen. Wichtig ist, dass $\sigma_{\text{SAMPLE}}$ so lange angepasst wird, bis die durchschnittliche Sampling Rate aller Gruppen wieder der ursprünglich festgelegten Rate von $q$ entspricht. So kann die Menge der gezogenen Punkte in einem Mini-Batch im Erwartungswert weiterhin gesteuert werden.

Die Funktion \texttt{getSampleRate} in dem Algorithmus bezieht sich auf eine Opacus-Funktion \cite{yousefpour:2021}. Sie berechnet die \textit{Sampling Rates} für ein Training mit homogenen Privacy-Budgets auf Grundlage der übergebenen Parameter.

Die Kombination aus dem \textit{Noise-Multiplier} und den individuellen \textit{sampling rates} sorgt dafür, dass die Privacy Budgets aller Datenpunkte der unterschiedlichen Privacy-Gruppen nach $I$ Iterationen aufgebraucht sind. 

Der \textit{Noise Multiplier} $\sigma$, mit dem $\sigma_{\text{SAMPLE}}$ zunächst initialisiert wird, entspricht dem Wert, den es bräuchte, wenn alle Datenpunkte das kleinste Privacy Budget, also $\epsilon_1$ hätten. Da aber die individualisierten Budgets teilweise größer sind, wird der Wert in jeder Iteration mit dem Faktor $0 < s_i < 1$ verringert.

\textcite[p.26f.]{boenisch:2023} beweisen in ihrer Arbeit, dass ihr Algorithmus $(\epsilon_p, \delta_p)$ Differential Privacy erfüllt. Ihr Beweis basiert auf \textcite{mironov:2017, mironov:2019}. 

%Sensitivität = 1 erwähnen
\textcite{mironov:2019} analysieren in ihrer Arbeit den \textit{Sampled Gaussian Mechanism} (SGM). Dieser Mechanismus beschreibt das Verfahren, in dem zufällig Beispiele gezogen werden, auf diesen eine Funktion ausgeführt wird und dann Rauschen aus der Normalverteilung addiert wird. Sie zeigen, dass ein SGM mit Sampling Rate $0 < q < 1$ und Standardabweichung $\sigma$ $(\alpha, \overline{\epsilon})$-Rényi Differential Privacy erfüllt mit $\overline{\epsilon} = 2q^2 \frac{\alpha}{\sigma^2}$.

\citeauthor{boenisch:2023} argumentieren, dass ihr \texttt{IDP-SGD} ein solcher SGM ist und übertragen den Beweis von \citeauthor{mironov:2019} direkt auf die Privacy-Parameter der Gruppen $\{q_1, ..., q_P\}\}$ und $\sigma_{\text{SAMPLE}}$. Mithilfe von \textcite{mironov:2017} können sie die Rényi-Differential Privacy in die klassische $(\epsilon, \delta)$-Form überführen.

Im Federated Learning verändern sich die Parameter. Der Server, auf dem Zwischenergebnisse verrauscht werden, zieht keine einzelnen Datenpunkte, sondern Gradienten verschiedener Clients. Dieser Prozess wiederholt sich über mehrere Iterationen, die Runden genannt werden. Die Iterationen im lokalen Trainingsprozess der Clients haben keinen Einfluss auf die Parameter mit denen die Updates verrauscht werden, was auch \textcite[p.3]{mcmahan:2018} als einen Vorteil von \texttt{FedAvg} gegenüber \texttt{FedSGD} werten. 

Um die Sampling Rates in meinem Federated Learning Algorithmus zu berechnen nutze ich daher ebenfalls \autoref{alg:boenisch-sample}, allerdings entsprechen die Zahl der Iterationen $I$ hier der Anzahl der Runden und die Anzahl der Datenpunkte $N$ ist die Anzahl der Clients. Wie auch \textcite{mcmahan:2018, boenisch:2023} wende ich zur Abschätzung der gesamten Privacy Kosten den \textit{moments accountant}, bzw. den \textit{rdp accountant} \cite{wang:2020} basierend auf Rényi Differential Privacy \cite{mironov:2017} an.

Die Argumentation dafür folgt \textcite{boenisch:2023}, denn auch mein Algorithmus zieht zufällig Beispiele, also Gradienten der Clients, mit einer \textit{Sampling Rate} $q_p$, berechnet darauf eine Funktion $f$, nämlich die Aggregation der Gradienten und addiert Rauschen aus der Normalverteilung. Daher ist er ebenfalls ein SGM. Die berechneten Parameter $q_p$ und $\sigma_{\text{SAMPLE}}$ für den SGM sind wie bei \citeauthor{boenisch:2023} mit dem RDP-Accountant berechnet und erfüllen damit die Privacy Anforderungen.

\section{Trainingsalgorithmus}
Ich habe mich dafür entschieden, \texttt{FedAvg} als Grundlage für das Training zu nutzen, da er weit verbreitet und in Federated Learning Frameworks implementiert ist. Auch \texttt{DP-FedAvg} mit globaler Differential Privacy \cite{mcmahan:2018} ist in der Regeln implementiert. Außerdem geht \textcite{mcmahan:2018} auf viele Punkte in Bezug auf Differential Privacy ein, die auch für meine Arbeit relevant sind. 

Insbesondere weisen \citeauthor{mcmahan:2018} darauf hin, dass die Sensitivität der Aggregation aus nicht sinnvoll beschränkt ist, wenn Clients zufällig gezogen werden. 

Sie gehen davon aus, dass jeder Client ein Gewicht $w_k$ zugewiesen bekommt, das proportional zu der Anzahl seiner Datenpunkte ist. Die Aggregation wäre also $\sum_{k \in \mathcal{C}} w_k \Delta_k$, wobei $\mathcal{C}$ die Menge der gezogenen Clients ist und $\Delta_k$ das Update, welches Client $k$ liefert. Um diese zu begrenzen schlagen sie zwei Schätzer vor, von denen $\tilde{f}_f(\mathcal{C}) = \frac{\sum_{k \in \mathcal{C}} w_k \Delta_k}{qW}$, mit $W = \sum_k{w_k}$, in Experimenten die besseren Ergebnisse liefert. Dieser hat eine Sensitivität $\S(\tilde{f}_f) \leq \frac{S}{qW}$, wobei $|w_k \Delta_k| \leq S$ für alle Clients $k$.

Die Sensitivität der Updates der einzelnen Clients gebe ich nicht direkt vor sondern nutze Adaptive Clipping \cite{andrew:2021}. Dabei wird statt einer festen Clipping Norm ein Quantil der Clients festgelegt, deren Updates geclippt werden sollen. Zur Abschätzung der dafür notwendigen Clipping Norm gibt jeder Client ein Bit zurück, das sagt, ob sein Update größer war als die aktuelle Norm oder nicht. Zur Abschätzung des geclippten Quantils wird dann die Summe der geclippten Updates mittels Differential Privacy berechnet. Dabei wird ein kleiner Teil des Privacy Budgets verwendet. Danach wird die Clipping Norm dementsprechend mit einer eigenen Learning Rate angepasst. 

Da die Berechnung der Anzahl der geclippten Updates selbst einen kleinen Teil des Privacy Budgets verbraucht, ist der Noise Multiplier für das Modelltraining leicht höher als der ursprünglich vorgegebene. Allerdings ist das Quantil der Updates, das gestutzt werden soll, ein Hyperparameter, der sich leichter optimieren lässt als die konkrete Clipping Norm, die potenziell abhängiger von dem Trainingsdatensatz sein kann.

Der Algorithmus ist in \autoref{alg:idp-fedavg} beschrieben. \texttt{DPFedAvgAdaptiveClipping} bezieht sich auf Algorithmus 1 aus \textcite[p.4]{andrew:2021} mit der Änderung, dass das Sampling der Clients nicht gleichverteilt passiert, sondern mit den individuellen Sampling Rates. Die Parameter, die sich auf das \textit{Adaptive Clipping} beziehen, habe ich in meiner Arbeit auf den Standardwerten belassen. Dabei ist das Quantil der Clients, deren Updates geclippt werden sollen der Median. Die weiteren Parameter dafür sind in im Repository von Tensorflow Federated zu finden.\footnote{\url{https://github.com/google-parfait/tensorflow-federated/blob/main/tensorflow_federated/python/aggregators/differential_privacy.py} (accessed 20-11-2024)}


\begin{algorithm}[tb]
	\caption{FederatedAveraging with individualized Differential Privacy (\texttt{IDP-FedAvg})}
	\label{alg:idp-fedavg}
	\SetKwFunction{IDPFedAvg}{IDPFedAvg}
	\SetKwFunction{DPFedAvgAdaptiveClipping}{DPFedAvgAdaptiveClipping}
	\SetKwFunction{GetSampleRates}{GetSampleRates}
	\SetKwFunction{Unique}{Unique}
	\SetKwProg{Fn}{Function}{:}{}
	\KwIn{Per-client target privacy budgets $\{\epsilon_1, ..., \epsilon_N\}$, target $\delta$, rounds $I$, expected clients per round $c$, client learning rate $\eta_c$, batch size $b$, client epochs $e$, server learning rate $\eta_s$, clipping quantile $\gamma$, clipping learning rate $\eta_C$, clipping noise multiplier $\sigma_b$, }
	\Fn{\IDPFedAvg{}}{
		$\mathcal{E}_G$ $\leftarrow$ \Unique{$\{\epsilon_1, ..., \epsilon_N\}$} \tcp*{different privacy groups}
		\tcp{see \autoref{alg:boenisch-sample}}
		$\sigma_{\text{SAMPLE}}$, $Q_G$ $\leftarrow$ \GetSampleRates{$\mathcal{E}_G$, $\delta$, $c$, $|\mathcal{C}|$, group sizes of $\mathcal{E}_G$}\;
		$\{q_1, ..., q_N\}$ $\leftarrow$ for each client in $\mathcal{C}$: get $q_i$ its privacy group from $Q_G$\;
		$\theta$ = \DPFedAvgAdaptiveClipping{$\{q_1, ..., q_N\}$, $\gamma$, $\eta_c$, $\eta_s$, $\eta_C$, $\sigma_{\text{SAMPLE}}$, $\sigma_b$}\;
		\Return{$\theta$}
	}
\end{algorithm}