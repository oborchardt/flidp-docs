\begin{chapter}{Structure}
	\begin{section}{Fundamentals}
		\textbf{Dieser Teil soll Grundlagen beschreiben (es kann ein B.Sc. Informatik vorausgesetzt werden). Es können Arbeiten referenziert werden, die existieren, aber nicht direkt meine Arbeit beeinflusst haben.}
		
		\subsection{Personalized Differential Privacy}
		\begin{itemize}
			\item \cite{alaggan:2016} und \cite{jorgensen:2015} haben Personalized Differential Privacy eingeführt (Alaggan kurz vor Jorgensen)
			\item Lucas nach dem Beweis von das Sample DP erfüllt in \cite{jorgensen:2015} fragen
			\item Sample Mechanismus aus \cite{jorgensen:2015} beschreiben und evtl mit \cite{boenisch:2023} vergleichen
			\begin{itemize}
				\item evtl auf Threshold $t$ eingehen und dabei auf die Trade-Off zwischen dem ausschließen von Daten durch einen geringen Threshold vs. dem Nutzen aller Daten aber dafür einem höheren Rauschen durch den DP-Algorithmus eingehen (bei höherem $t$)?
			\end{itemize}
		\end{itemize}
		
		\subsection{Federated Learning}
		\begin{itemize}
			\item Genereller Ablauf (Server initialisiert Parameter, schickt die an Clients, ...)
			\item \texttt{FedAvg} und \texttt{FedSGD} als Standardalgorithmen
			\item cross-silo vs fully distributed (unterschiedliche Szenarien bzw Clients, zb Forschungsgruppen / datacenter vs Smartphone User)
			\item besondere Herausforderungen im FL (siehe vor allem \cite{kairouz:2021})
			\begin{itemize}
				\item Non-iid Data (und dessen mögliche Formen)
				\item Verfügbarkeit von Clients
				\item Menge der über das Netzwerk zu transportierenden Daten
				\item gute Debugging / Monitoring-Lösungen
			\end{itemize}
			\item Privacy model in Federated Learning
			\begin{itemize}
				\item global vs lokal
				\item record-level vs user-level
			\end{itemize}
		\end{itemize}
		
	\end{section}
	
	\begin{section}{Related Work}
		\textbf{Dieser Teil soll Arbeiten referenzieren, die direkt meine Arbeit beeinflusst haben. Dazu zählen auch Arbeiten die die gewählten Parameter rechtfertigen (Verteilung der Budgets, die Budgets selbst, ...)}
		
		\subsection{Wahl der Parameter}
		\subsubsection{Wahl der Privacy-Budgets}
		\begin{itemize}
			\item Wahl zunächst wie bei \cite{boenisch:2023}
			\item \cite{sun:2021} legt Nahe, dass die größe der Budgets um gut trainieren zu können stark von den Datensätzen abhängt
			\begin{itemize}
				\item bei mnist gehen sie von $\epsilon > 0.3$ aus während sie für CIFAR10 erst bei $\epsilon > 5$ sagen, dass ein Großteil der Accuracy beibehalten wird
			\end{itemize}
		\end{itemize}
		\subsubsection{Verteilung der einzelnen Privacy-Gruppen}
		\begin{itemize}
			\item \cite{alaggan:2016} für individual-relaxed Verteilung der Budgets
				\begin{itemize}
					\item auf S.15f beschreiben sie, dass sich die Privacy Einstellungen von Nutzern in 3 Gruppen einteilen lassen (Fundamentalists, Pragmatists und Unconcerned)
					\item \cite{jensen:2005} wird als Beleg für die 0.34, 0.43, 0.23 Verteilung genannt (individual-strict)
					\item \cite{acquisti:2005} ist eine Umfrage, die die weniger strikte Verteilung nahelegt (0.54, 0.37, 0.09) (individual-relaxed)
				\end{itemize}
		\end{itemize}
		
		\subsection{IDP in ML}
		
		\subsection{DP in FL}
		
		\subsection{IDP in FL}
		
		\subsection{Algorithmus}
		\begin{itemize}
			\item \cite{boenisch:2023} für den Sampling Ansatz
			\item \cite{mcmahan:2016} für FedAvg
			\item \cite{mcmahan:2018} für FedAvg mit DP
			\item \cite{aldaghri:2023} für ein anderes Verfahren mit individualisierter DP im Federated Learning?
			\begin{itemize}
				\item entspricht eher dem Scale Ansatz von \cite{boenisch:2023}
				\item nur mit zwei \glqq{}Privacy Niveaus\grqq{} getestet
			\end{itemize}
		\end{itemize}
		
		\subsection{Ergebnisse anderer Paper auf meinen Datensätzen}
		
		\begin{itemize}
			\item cifar-10, mnist \cite{sun:2021}
		\end{itemize}
	\end{section}
\end{chapter}