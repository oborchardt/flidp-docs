\begin{chapter}{Structure}
	\begin{section}{Fundamentals}
		\textbf{Dieser Teil soll Grundlagen beschreiben (es kann ein B.Sc. Informatik vorausgesetzt werden). Es können Arbeiten referenziert werden, die existieren, aber nicht direkt meine Arbeit beeinflusst haben.}
		
		\subsection{Differential Privacy}
		\begin{itemize}
			\item Anwendung von DP in the wild (\cite{erlingsson:2014, tezapsidis:2017, apple:2017})
			\item zunächst beschreiben was Privacy ist und wo sich DP einordnen lässt? (interactive, anstatt non-interactive wie k-anonymity)
			\item Randomized Response als anschauliches Beispiel? (ähnlich wie bei \cite[p.1]{erlingsson:2014})
			\item Attacken auf Privatheit beschreiben, gegen die DP hilft? (wie in der Introduction von \cite{abadi:2016})
			\item 2006 vorgeschlagen von \cite{dwork:2006}
			\item Exponential Mechanism vs andere Mechanismen -> mit ihm kann man alle DP-Mechanismen beschreiben? (\url{https://en.wikipedia.org/wiki/Exponential_mechanism})
			\item Kompositionstheoreme
			\item Renyi Differential Privacy\cite{mironov:2017}
		\end{itemize}
		
		\subsection{DP in ML}
		\begin{itemize}
			\item Warum nutzt man DP, was ist die Motivation -> Verteidigung gegen Model Inversion Attacks(?), ...
			\item was sind Variablen beim Training, die einen Einfluss auf das benötigte Privacy Budget haben? (Da kann dann eine Brücke zum FL geschlagen werden)
		\end{itemize}
		
		\subsection{Personalized Differential Privacy}
		\begin{itemize}
			\item \cite{alaggan:2016} und \cite{jorgensen:2015} haben Personalized Differential Privacy eingeführt (Alaggan kurz vor Jorgensen)
			\item Lucas nach dem Beweis von das Sample DP erfüllt in \cite{jorgensen:2015} fragen
			\item Sample Mechanismus aus \cite{jorgensen:2015} beschreiben und evtl mit \cite{boenisch:2023} vergleichen
			\begin{itemize}
				\item evtl auf Threshold $t$ eingehen und dabei auf die Trade-Off zwischen dem ausschließen von Daten durch einen geringen Threshold vs. dem Nutzen aller Daten aber dafür einem höheren Rauschen durch den DP-Algorithmus eingehen (bei höherem $t$)?
			\end{itemize}
		\end{itemize}
		
		\subsection{Federated Learning}
		\begin{itemize}
			\item Genereller Ablauf (Server initialisiert Parameter, schickt die an Clients, ...)
			\item \texttt{FedAvg} und \texttt{FedSGD} als Standardalgorithmen
			\item cross-silo vs fully distributed (unterschiedliche Szenarien bzw Clients, zb Forschungsgruppen / datacenter vs Smartphone User)
			\item besondere Herausforderungen im FL (siehe vor allem \cite{kairouz:2021})
			\begin{itemize}
				\item Non-iid Data (und dessen mögliche Formen)
				\item Verfügbarkeit von Clients
				\item Menge der über das Netzwerk zu transportierenden Daten
				\item gute Debugging / Monitoring-Lösungen
			\end{itemize}
			\item Privacy model in Federated Learning
			\begin{itemize}
				\item global vs lokal
				\item record-level vs user-level
			\end{itemize}
		\end{itemize}
		
	\end{section}
	
	\begin{section}{Related Work}
		\textbf{Dieser Teil soll Arbeiten referenzieren, die direkt meine Arbeit beeinflusst haben. Dazu zählen auch Arbeiten die die gewählten Parameter rechtfertigen (Verteilung der Budgets, die Budgets selbst, ...)}
		
		\subsection{Wahl der Parameter}
		\subsubsection{Wahl der Privacy-Budgets}
		\begin{itemize}
			\item Wahl zunächst wie bei \cite{boenisch:2023}
			\item \cite{sun:2021} legt Nahe, dass die größe der Budgets um gut trainieren zu können stark von den Datensätzen abhängt
			\begin{itemize}
				\item bei mnist gehen sie von $\epsilon > 0.3$ aus während sie für CIFAR10 erst bei $\epsilon > 5$ sagen, dass ein Großteil der Accuracy beibehalten wird
			\end{itemize}
		\end{itemize}
		\subsubsection{Verteilung der einzelnen Privacy-Gruppen}
		\begin{itemize}
			\item \cite{alaggan:2016} für individual-relaxed Verteilung der Budgets
				\begin{itemize}
					\item auf S.15f beschreiben sie, dass sich die Privacy Einstellungen von Nutzern in 3 Gruppen einteilen lassen (Fundamentalists, Pragmatists und Unconcerned)
					\item \cite{jensen:2005} wird als Beleg für die 0.34, 0.43, 0.23 Verteilung genannt (individual-strict)
					\item \cite{acquisti:2005} ist eine Umfrage, die die weniger strikte Verteilung nahelegt (0.54, 0.37, 0.09) (individual-relaxed)
				\end{itemize}
		\end{itemize}
		
		\subsection{IDP in ML}
		
		\subsection{DP in FL}
		
		\subsection{IDP in FL}
		
		\subsection{Algorithmus}
		\begin{itemize}
			\item \cite{boenisch:2023} für den Sampling Ansatz
			\item \cite{mcmahan:2016} für FedAvg
			\item \cite{mcmahan:2018} für FedAvg mit DP
			\item \cite{aldaghri:2023} für ein anderes Verfahren mit individualisierter DP im Federated Learning?
			\begin{itemize}
				\item entspricht eher dem Scale Ansatz von \cite{boenisch:2023}
				\item nur mit zwei \glqq{}Privacy Niveaus\grqq{} getestet
			\end{itemize}
		\end{itemize}
		
		\subsection{Ergebnisse anderer Paper auf meinen Datensätzen}
		
		\begin{itemize}
			\item cifar-10, mnist \cite{sun:2021}
		\end{itemize}
	\end{section}
\end{chapter}